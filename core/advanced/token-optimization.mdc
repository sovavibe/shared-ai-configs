---
description: 'Token optimization - 60-70% reduction target via progressive loading'
alwaysApply: false
---

# @127 Token Optimization

## Baseline vs Target

| Aspect | Before (2025) | Target (2026) | Method |
|--------|---------------|---------------|--------|
| Session overhead | 5-8K tokens | 3-5K tokens | Progressive loading |
| MCP discovery | 8K tokens | <2K tokens | Tool Search feature |
| Rules loaded | 3-4K tokens | 1-2K tokens | Glob-based auto-load |
| **Total context budget** | **~15-20K** | **~5-8K** | **60-70% reduction** |

## Strategy 1: Glob-Based Auto-Loading

**Problem**: All 41 rules loaded even if only 1-2 apply to current file

**Solution**: Load rules matching file type

```yaml
# shared-ai-configs/core/INDEX.mdc updates:
010-049:  # Context rules - auto-load by glob
  - glob: "src/**/*.tsx"
    rules: [@tanstack-query, @ant-design]
  - glob: "src/**/store/**"
    rules: [@zustand]
  - glob: "**/*styled*"
    rules: [@styling]
```

**Result**: Load only 2-4 rules per file (instead of 41)
**Savings**: 3-4K tokens per session

## Strategy 2: MCP Tool Search

**Problem**: All MCPs preloaded in mcp.json (51K tokens)

**Solution**: Use Tool Search to dynamically load only needed tools

```bash
# Instead of loading all MCPs:
# Load only what you need
# Tool Search: "@GitHub @Context7 @Snyk"
```

**Result**: Load only 3-5 MCPs (instead of 10+)
**Savings**: 6-8K tokens per session

## Strategy 3: Progressive Loading

**Pattern**:

1. Core context loads (essential only) - 2-3K
2. File-specific rules load on @mention - auto
3. On-demand MCPs load when needed - auto

**Benefits**:

- Fast session start
- Automatic expansion
- No manual tool switching

## Strategy 4: Decision Tree Compression

**Before** (Explanation format - 150+ tokens):

```
When implementing authentication, consider:
- Session-based: Store token in session...
- JWT: Create JWT payload...
- OAuth: Use provider...
- Recommendation: Use JWT because [explanation]
```

**After** (Decision table - 20 tokens, 87% savings):

```markdown
| Method | When | Pros | Cons |
|--------|------|------|------|
| JWT | Stateless APIs | Fast, scalable | Complex revocation |
| Session | Traditional | Simple, secure | Server state |
| OAuth | Third-party | Delegated auth | Dependency |

**Choose**: JWT for REST APIs, Session for traditional web
```

## Strategy 5: Multi-Turn Context Persistence (continuation_id)

**Problem**: Each PAL MCP call loses previous conversation context (redundant re-explanation)

**Solution**: Use `continuation_id` to maintain thread context across tool calls

```bash
# First call - establishes context
mcp__pal__thinkdeep
  step="Analyzing authentication flow"
  continuation_id="auth-investigation-001"
  ...

# Subsequent calls - reuse context (no re-explanation needed)
mcp__pal__thinkdeep
  step="Found issue in token refresh"
  continuation_id="auth-investigation-001"  # Same ID!
  ...
```

**Pattern Rules**:

- Generate unique ID per investigation thread (e.g., `<topic>-<timestamp>`)
- Reuse ID for ALL related calls within same thread
- Works across different PAL tools (thinkdeep, debug, codereview, precommit)
- Context preserved: files, findings, hypotheses

**Savings**: 2-4K tokens per multi-step investigation (no context rebuilding)

## Strategy 6: Memory Bank Integration

**Problem**: Session findings lost, next session rebuilds from scratch

**Solution**: Persist analysis to Memory Bank, recall on demand

```bash
# Save findings (end of investigation)
mcp__allpepper-memory-bank__memory_bank_write
  projectName="<project>"
  fileName="analysis-<topic>.md"
  content="## Findings\n- [key insights]"

# Recall later (next session)
mcp__allpepper-memory-bank__memory_bank_read
  projectName="<project>"
  fileName="analysis-<topic>.md"
```

**Storage Strategy**:

| Data Type | Storage | Why |
|-----------|---------|-----|
| Session findings | Memory Bank | Cross-session reuse |
| Important decisions | Hindsight `retain` | Auto-recalled via semantic search |
| Task tracking | beads | Issue database with dependencies |
| Reference docs | Git (docs/) | Version-controlled, team-shared |

**Savings**: 3-5K tokens per session (no re-analysis of known patterns)

## Strategy 7: Hindsight Memory Patterns

**Three operations for long-term memory**:

```bash
# RETAIN - Save important decision
mcp__hindsight-alice__retain "Decision: Use JWT for auth because [reason]"

# RECALL - Retrieve past context (semantic search)
mcp__hindsight-alice__recall "How do we handle authentication?"

# REFLECT - Deep synthesis across memories
mcp__hindsight-alice__reflect "What patterns emerged in error handling?"
```

**When to use each**:

| Operation | When | Token Impact |
|-----------|------|--------------|
| `retain` | After making important decision | +50 tokens (save) |
| `recall` | Before starting related work | -500-2K (avoid re-discovery) |
| `reflect` | Complex problems, multiple sources | -1-3K (synthesized context) |

**Pattern**: Retain at session end, Recall at session start

## Strategy 8: Context Window Management

**Problem**: Long sessions exhaust context window (200K limit approached)

**Mitigation Techniques**:

```mermaid
graph TD
    A["Session Start<br/>~5K tokens"] --> B["Work Phase<br/>Accumulating"]
    B --> C{">100K tokens?"}
    C -->|No| B
    C -->|Yes| D["Compact Context"]
    D --> E["Summarize findings"]
    E --> F["Clear intermediate"]
    F --> G["Continue<br/>~30K tokens"]
```

**Commands**:

```bash
# Built-in compaction
/compact  # Summarizes and reduces context

# Manual checkpoint
mcp__hindsight-alice__retain "Checkpoint: [summary of work so far]"
# Then start fresh session with recall
```

**Threshold Guidelines**:

| Context Used | Action |
|--------------|--------|
| <50% (100K) | Continue normally |
| 50-75% | Consider /compact |
| >75% | Checkpoint + new session |

## Strategy 9: Auto-Summarization Patterns

**Problem**: Verbose tool outputs consume tokens rapidly

**Solution**: Request summaries, not full outputs

```bash
# Instead of reading entire file:
mcp__pal__thinkdeep
  step="Summarize key patterns in auth module"
  # Returns synthesis, not raw content

# Instead of full git diff:
# Use focused queries
git diff --stat  # Overview first
git diff <specific-file>  # Then drill down
```

**Summarization Hierarchy**:

1. **Overview first** - Get structure, not content
2. **Targeted deep-dive** - Only expand what matters
3. **Progressive detail** - Add context incrementally

**Tool-Specific Patterns**:

| Tool | Verbose | Optimized |
|------|---------|-----------|
| Read | Full file | Read with offset/limit |
| Grep | All matches | head_limit parameter |
| git diff | Full diff | --stat then specific files |
| PAL tools | Full analysis | Focused step descriptions |

## Implementation Checklist

- [ ] Audit current rules → identify unused (remove)
- [ ] Add glob-based auto-loading to INDEX.mdc
- [ ] Test file-specific rule loading
- [ ] Set up Tool Search for MCPs
- [ ] Convert decision explanations to tables
- [ ] Implement continuation_id for multi-turn investigations
- [ ] Set up Memory Bank for session persistence
- [ ] Configure Hindsight retain/recall workflow
- [ ] Establish context checkpoint thresholds
- [ ] Measure token usage (baseline → target)

## Verification

```bash
# Before optimization (2025):
# Session starts, 5-8K overhead
# Multi-turn rebuilds context each call
# Findings lost between sessions

# After optimization:
# Session starts, 3-5K overhead
# continuation_id preserves thread context
# Memory Bank + Hindsight for cross-session recall
# 60-70% reduction achieved
```

---

**Enforce this rule**: Always measure token usage, target 3-5K overhead per session. Use continuation_id for multi-turn, Memory Bank for persistence.
